{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Speed\n",
    "\n",
    "## General Notes\n",
    "\n",
    "\n",
    "- All previous comments about naming files, tags and readable commit messages still apply.\n",
    "- While testing is not the main focus of this assignment, you will have to write some tests. As a general rule: Never try to improve the speed of a function that does not have unit tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assignment four, you implemented a function called `square_root_linear_update` that updates a previous state estimate with one new measurement. Since not all solutions to assignment four were correct, we provide you with one possible solution that you can find in `code/update.py`. Note that there are many other possible solutions that are equally good. \n",
    "\n",
    "This solution will also serve as a reference for speed improvements. We renamed the function to `pandas_update` to contrast it from other implementations you will program during this assignment.\n",
    "\n",
    "If Kalman filters are used to estimate the parameters of The Technology of Skill Formation, the update step has to be carried out repeatedly for all observations in a dataset. This can be achieved by calling the above function in a loop. `code/update.py` contains a function called `pandas_batch_update` that does this. \n",
    "\n",
    "You can see that in `pandas_batch_update` most inputs got one dimension more than before. This was problematic in the case of root_cov, since it already was a DataFrame. Therefore, we used lists of DataFrames. \n",
    "\n",
    "Unfortunately, `pandas_batch_update` is extremely slow. Your main task during this assignment is to make it fast. The group that produces the fastest implementation wins a case of beer that can be picked up on January 17, 6 p.m. at the IAME lounge. \n",
    "\n",
    "To measure the speed of the pandas function and your function, we use the data from Cunha, Heckman and Schennach 2010 you already know from assignment 3. The initial state estimate is zero for every individual. The initial covariance is also identical for all individuals and can be constructed from estimated parameters. \n",
    "\n",
    "For the benchmark we are going to update the initial state estimate with the first measurement of cognitive skills (birthweight). The factor loading for this measurement was normalized to 1. Unfortunately, the parameters of the measurement variance are not reported, so we just fix a value.\n",
    "\n",
    "For simplicity, we filled all missing observations with the average birthweight. This is not necessary for estimation, but writing a Kalman update that can handle missing data and is fast is too difficult for this assignment.\n",
    "\n",
    "The speed of your function will be measured on the same data and in the same way as we measure the speed of `pandas_batch_update.py` in timing.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Clone this repository, add a .gitignore-file as in previous assignments, and create branches with the github names of each group member. As last time, each of you will do work on his or her branch and occasionally merge the result into the master branch. This time you don't need a .tex file for the written solution.\n",
    "\n",
    "2. Run `timing.py`. It will print out the runtime of `pandas_batch_update`. It took about 5 seconds on our laptop, but might be faster or slower on yours. \n",
    "\n",
    "3. Adjust your tests from last time for a function called `fast_batch_update` with the following interface:\n",
    "\n",
    "    ```python\n",
    "    def fast_batch_update(states, root_covs, measurements, loadings, meas_var):\n",
    "        \"\"\"Update state estimates for a whole dataset.\n",
    "        \n",
    "        Let nstates be the number of states and nobs the number of observations.\n",
    "        \n",
    "        Args:\n",
    "            states (np.ndarray): 2d array of size (nobs, nstates)\n",
    "            root_covs (np.ndarray): 3d array of size (nobs, nstates, nstates)\n",
    "            measurements (np.ndarray): 1d array of size (nobs)\n",
    "            loadings (np.ndarray): 1d array of size (nstates)\n",
    "            meas_var (float):\n",
    "        \n",
    "        Returns:\n",
    "            updated_states (np.ndarray): 2d array of size (nobs, nstates)\n",
    "            updated_root_covs (np.ndarray): 3d array of size (nobs, nstates, nstates)\n",
    "        \n",
    "        \"\"\"\n",
    "    ```\n",
    "    \n",
    "    If you are unsure wheter your tests last time were correct, you can generate new test cases with the `pandas_update` function we provided. As last time, all tests should be defined in `test_update.py`.\n",
    "\n",
    "4. Implement the function `fast_batch_update` in the module `update.py` and make it as fast as possible. Extend `timing.py` such that it also measures and prints the runtime of `fast_batch_update`. \n",
    "\n",
    "    There are no other constraints. You can use any of the tools you saw in the lectures (numpy, numba, numexpr, ...). We encourage you to try out several tools. You can leave your trials in the code and print their runtime in `timing.py`, so we can see what you did. \n",
    "\n",
    "    We expect you to achieve at least a 100 times speed-up over the pandas implementation. A 500 times speedup will probably give you a good chance to win. If you achieve a speed-up of more than 1000 we will be impressed (but it is definitely possible!). \n",
    "\n",
    "5. Once you are satisfied with your solution, merge it into the master branch of your repository. Then add a tag called `exercise_5`  and push all branches (including the master branch and the tag) to the central server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- If you do the calculations in numpy, it is usually faster to use operations that transform the full arrays instead of looping and selecting the data for each observation.\n",
    "- If you use numba, it can sometimes help to write out all dot products as loops. But this makes the code ugly and is a last resort. \n",
    "- Always use numba in nopython mode\n",
    "- Numba only supports the QR decomposition from numpy.linalg and only if you don't use any of the optional arguments. The one from scipy.linalg does not work in nopython mode. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
